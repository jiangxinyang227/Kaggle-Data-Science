{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6894</td>\n",
       "      <td>Hopefully the score has changed by now due to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8263</td>\n",
       "      <td>(Spoilers galore) This is an absolutely awful ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11776</td>\n",
       "      <td>Jack Black can usually make me snicker simply ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11096</td>\n",
       "      <td>After seeing all the Jesse James, Quantrill, j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12423</td>\n",
       "      <td>Make no mistake, Maureen O'Sullivan is easily ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  sentiment\n",
       "0   6894  Hopefully the score has changed by now due to ...          1\n",
       "1   8263  (Spoilers galore) This is an absolutely awful ...          0\n",
       "2  11776  Jack Black can usually make me snicker simply ...          0\n",
       "3  11096  After seeing all the Jesse James, Quantrill, j...          1\n",
       "4  12423  Make no mistake, Maureen O'Sullivan is easily ...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_data(dir):\n",
    "    data = {}\n",
    "    data['sentence'] = []\n",
    "    for file in os.listdir(dir):\n",
    "        with open(os.path.join(dir, file), 'r') as f:\n",
    "            data['sentence'].append(f.read())\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "def load_data(dir):\n",
    "    pos_df = download_data(os.path.join(dir, 'pos'))\n",
    "    neg_df = download_data(os.path.join(dir, 'neg'))\n",
    "    pos_df['sentiment'] = 1\n",
    "    neg_df['sentiment'] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=False)\n",
    "\n",
    "train_df = load_data('/home/jiangxinyang/git_projects/machine_learning/tensorflow/Sequences/aclImdb/train')\n",
    "test_df = load_data('/home/jiangxinyang/git_projects/machine_learning/tensorflow/Sequences/aclImdb/test')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxinyang/.virtualenvs/machine_learning-_zdHOwB3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/jiangxinyang/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html5lib\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hopefully',\n",
       " 'the',\n",
       " 'score',\n",
       " 'has',\n",
       " 'changed',\n",
       " 'by',\n",
       " 'now',\n",
       " 'due',\n",
       " 'to',\n",
       " 'my',\n",
       " 'brilliant',\n",
       " 'and',\n",
       " 'stunning',\n",
       " 'review',\n",
       " 'which',\n",
       " 'persuades',\n",
       " 'all',\n",
       " 'of',\n",
       " 'you',\n",
       " 'to',\n",
       " 'go',\n",
       " 'and',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'film',\n",
       " 'thereby',\n",
       " 'creating',\n",
       " 'an',\n",
       " 'instant',\n",
       " 'chorus',\n",
       " 'of',\n",
       " 's',\n",
       " 'this',\n",
       " 'movie',\n",
       " 's',\n",
       " 'true',\n",
       " 'score',\n",
       " 'as',\n",
       " 'mentioned',\n",
       " 'before',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'the',\n",
       " 'king',\n",
       " 'previous',\n",
       " 'to',\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'i',\n",
       " 'wasn',\n",
       " 't',\n",
       " 'that',\n",
       " 'over',\n",
       " 'the',\n",
       " 'top',\n",
       " 'about',\n",
       " 'him',\n",
       " 'but',\n",
       " 'now',\n",
       " 'i',\n",
       " 'm',\n",
       " 'banging',\n",
       " 'on',\n",
       " 'the',\n",
       " 'doors',\n",
       " 'of',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 's',\n",
       " 'website',\n",
       " 'begging',\n",
       " 'him',\n",
       " 'to',\n",
       " 'take',\n",
       " 'me',\n",
       " 'on',\n",
       " 'as',\n",
       " 'his',\n",
       " 'protege',\n",
       " 'this',\n",
       " 'film',\n",
       " 'is',\n",
       " 'truly',\n",
       " 'funny',\n",
       " 'if',\n",
       " 'you',\n",
       " 'don',\n",
       " 't',\n",
       " 'find',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'funny',\n",
       " 'you',\n",
       " 'really',\n",
       " 'need',\n",
       " 'therapy',\n",
       " 'and',\n",
       " 'it',\n",
       " 's',\n",
       " 'humour',\n",
       " 'which',\n",
       " 'targets',\n",
       " 'all',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'society',\n",
       " 'including',\n",
       " 'race',\n",
       " 'predictably',\n",
       " 'class',\n",
       " 'division',\n",
       " 'love',\n",
       " 'wealth',\n",
       " 'employment',\n",
       " 'dreams',\n",
       " 'stand',\n",
       " 'up',\n",
       " 'comedy',\n",
       " 'the',\n",
       " 'list',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'there',\n",
       " 'was',\n",
       " 'one',\n",
       " 'slight',\n",
       " 'disappointment',\n",
       " 'for',\n",
       " 'me',\n",
       " 'however',\n",
       " 'this',\n",
       " 'was',\n",
       " 'that',\n",
       " 'in',\n",
       " 'going',\n",
       " 'into',\n",
       " 'this',\n",
       " 'film',\n",
       " 'i',\n",
       " 'didn',\n",
       " 't',\n",
       " 'realise',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'actually',\n",
       " 'a',\n",
       " 'remake',\n",
       " 'of',\n",
       " 'heaven',\n",
       " 'can',\n",
       " 'wait',\n",
       " 'another',\n",
       " 'quite',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'in',\n",
       " 'with',\n",
       " 'warren',\n",
       " 'beatty',\n",
       " 'as',\n",
       " 'such',\n",
       " 'i',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'surprised',\n",
       " 'when',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'suddenly',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'began',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'to',\n",
       " 'be',\n",
       " 'distinctly',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'an',\n",
       " 'older',\n",
       " 'movie',\n",
       " 'i',\n",
       " 'had',\n",
       " 'watched',\n",
       " 'on',\n",
       " 'tv',\n",
       " 'a',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'regardless',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'in',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'the',\n",
       " 'better',\n",
       " 'version',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'simply',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'different',\n",
       " 'areas',\n",
       " 'it',\n",
       " 'covers',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'funnier',\n",
       " 'than',\n",
       " 'warren',\n",
       " 'beatty',\n",
       " 'any',\n",
       " 'second',\n",
       " 'of',\n",
       " 'any',\n",
       " 'day',\n",
       " 'of',\n",
       " 'any',\n",
       " 'week',\n",
       " 'of',\n",
       " 'any',\n",
       " 'year',\n",
       " 'of',\n",
       " 'any',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'well',\n",
       " 'to',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'plot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'don',\n",
       " 't',\n",
       " 'spoil',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'yourself',\n",
       " 'don',\n",
       " 't',\n",
       " 'read',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'just',\n",
       " 'go',\n",
       " 'and',\n",
       " 'watch',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'because',\n",
       " 'there',\n",
       " 'have',\n",
       " 'been',\n",
       " 'two',\n",
       " 'reviews',\n",
       " 'on',\n",
       " 'imdb',\n",
       " 'so',\n",
       " 'far',\n",
       " 'that',\n",
       " 'have',\n",
       " 'raved',\n",
       " 'mad',\n",
       " 'about',\n",
       " 'it',\n",
       " 'go',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'funniest',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'would',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'in',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'go',\n",
       " 'and',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 's',\n",
       " 'a',\n",
       " 'cinema',\n",
       " 'experience',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'leave',\n",
       " 'you',\n",
       " 'grumbling',\n",
       " 'ad',\n",
       " 'nauseum',\n",
       " 'at',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'tickets',\n",
       " 'go',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'movie']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_df = download_data('/home/jiangxinyang/git_projects/machine_learning/tensorflow/Sequences/aclImdb/train/unsup')\n",
    "def sentence_to_wordList(sentence):\n",
    "    sentence_text = BeautifulSoup(sentence).get_text()\n",
    "    sentence_text = re.sub('[^a-zA-Z]', \" \", sentence_text)\n",
    "    word_list = sentence_text.lower().split()\n",
    "    \n",
    "    return word_list\n",
    "\n",
    "sentences = []\n",
    "for sentence in train_df['sentence']:\n",
    "    sentences.append(sentence_to_wordList(sentence))\n",
    "\n",
    "for sentence in unlabeled_df['sentence']:\n",
    "    sentences.append(sentence_to_wordList(sentence))\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopefully',\n",
       " 'the',\n",
       " 'score',\n",
       " 'has',\n",
       " 'changed',\n",
       " 'by',\n",
       " 'now',\n",
       " 'due',\n",
       " 'to',\n",
       " 'my',\n",
       " 'brilliant',\n",
       " 'and',\n",
       " 'stunning',\n",
       " 'review',\n",
       " 'which',\n",
       " 'persuades',\n",
       " 'all',\n",
       " 'of',\n",
       " 'you',\n",
       " 'to',\n",
       " 'go',\n",
       " 'and',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'film',\n",
       " 'thereby',\n",
       " 'creating',\n",
       " 'an',\n",
       " 'instant',\n",
       " 'chorus',\n",
       " 'of',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'true',\n",
       " 'score',\n",
       " 'as',\n",
       " 'mentioned',\n",
       " 'before',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'the',\n",
       " 'king',\n",
       " 'previous',\n",
       " 'to',\n",
       " 'going',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'wasn',\n",
       " 'that',\n",
       " 'over',\n",
       " 'the',\n",
       " 'top',\n",
       " 'about',\n",
       " 'him',\n",
       " 'but',\n",
       " 'now',\n",
       " 'banging',\n",
       " 'on',\n",
       " 'the',\n",
       " 'doors',\n",
       " 'of',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 'website',\n",
       " 'begging',\n",
       " 'him',\n",
       " 'to',\n",
       " 'take',\n",
       " 'me',\n",
       " 'on',\n",
       " 'as',\n",
       " 'his',\n",
       " 'protege',\n",
       " 'this',\n",
       " 'film',\n",
       " 'is',\n",
       " 'truly',\n",
       " 'funny',\n",
       " 'if',\n",
       " 'you',\n",
       " 'don',\n",
       " 'find',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'funny',\n",
       " 'you',\n",
       " 'really',\n",
       " 'need',\n",
       " 'therapy',\n",
       " 'and',\n",
       " 'it',\n",
       " 'humour',\n",
       " 'which',\n",
       " 'targets',\n",
       " 'all',\n",
       " 'areas',\n",
       " 'of',\n",
       " 'society',\n",
       " 'including',\n",
       " 'race',\n",
       " 'predictably',\n",
       " 'class',\n",
       " 'division',\n",
       " 'love',\n",
       " 'wealth',\n",
       " 'employment',\n",
       " 'dreams',\n",
       " 'stand',\n",
       " 'up',\n",
       " 'comedy',\n",
       " 'the',\n",
       " 'list',\n",
       " 'goes',\n",
       " 'on',\n",
       " 'there',\n",
       " 'was',\n",
       " 'one',\n",
       " 'slight',\n",
       " 'disappointment',\n",
       " 'for',\n",
       " 'me',\n",
       " 'however',\n",
       " 'this',\n",
       " 'was',\n",
       " 'that',\n",
       " 'in',\n",
       " 'going',\n",
       " 'into',\n",
       " 'this',\n",
       " 'film',\n",
       " 'didn',\n",
       " 'realise',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'actually',\n",
       " 'remake',\n",
       " 'of',\n",
       " 'heaven',\n",
       " 'can',\n",
       " 'wait',\n",
       " 'another',\n",
       " 'quite',\n",
       " 'good',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'in',\n",
       " 'with',\n",
       " 'warren',\n",
       " 'beatty',\n",
       " 'as',\n",
       " 'such',\n",
       " 'was',\n",
       " 'quite',\n",
       " 'surprised',\n",
       " 'when',\n",
       " 'watched',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'suddenly',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'began',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'to',\n",
       " 'be',\n",
       " 'distinctly',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'an',\n",
       " 'older',\n",
       " 'movie',\n",
       " 'had',\n",
       " 'watched',\n",
       " 'on',\n",
       " 'tv',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'ago',\n",
       " 'regardless',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'in',\n",
       " 'my',\n",
       " 'opinion',\n",
       " 'the',\n",
       " 'better',\n",
       " 'version',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'two',\n",
       " 'of',\n",
       " 'them',\n",
       " 'simply',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'different',\n",
       " 'areas',\n",
       " 'it',\n",
       " 'covers',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'chris',\n",
       " 'rock',\n",
       " 'is',\n",
       " 'funnier',\n",
       " 'than',\n",
       " 'warren',\n",
       " 'beatty',\n",
       " 'any',\n",
       " 'second',\n",
       " 'of',\n",
       " 'any',\n",
       " 'day',\n",
       " 'of',\n",
       " 'any',\n",
       " 'week',\n",
       " 'of',\n",
       " 'any',\n",
       " 'year',\n",
       " 'of',\n",
       " 'any',\n",
       " 'you',\n",
       " 'get',\n",
       " 'the',\n",
       " 'picture',\n",
       " 'well',\n",
       " 'to',\n",
       " 'the',\n",
       " 'actual',\n",
       " 'plot',\n",
       " 'of',\n",
       " 'the',\n",
       " 'film',\n",
       " 'don',\n",
       " 'spoil',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'yourself',\n",
       " 'don',\n",
       " 'read',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'just',\n",
       " 'go',\n",
       " 'and',\n",
       " 'watch',\n",
       " 'movie',\n",
       " 'because',\n",
       " 'there',\n",
       " 'have',\n",
       " 'been',\n",
       " 'two',\n",
       " 'reviews',\n",
       " 'on',\n",
       " 'imdb',\n",
       " 'so',\n",
       " 'far',\n",
       " 'that',\n",
       " 'have',\n",
       " 'raved',\n",
       " 'mad',\n",
       " 'about',\n",
       " 'it',\n",
       " 'go',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'funniest',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'would',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'in',\n",
       " 'long',\n",
       " 'time',\n",
       " 'go',\n",
       " 'and',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 'cinema',\n",
       " 'experience',\n",
       " 'that',\n",
       " 'doesn',\n",
       " 'leave',\n",
       " 'you',\n",
       " 'grumbling',\n",
       " 'ad',\n",
       " 'nauseum',\n",
       " 'at',\n",
       " 'the',\n",
       " 'cost',\n",
       " 'of',\n",
       " 'cinema',\n",
       " 'tickets',\n",
       " 'go',\n",
       " 'see',\n",
       " 'it',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'good',\n",
       " 'movie']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [[word for word in sentence if len(word) > 1] for sentence in sentences]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-03 18:40:05,543: INFO: collecting all words and their counts\n",
      "2018-07-03 18:40:05,545: INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-07-03 18:40:06,161: INFO: PROGRESS: at sentence #10000, processed 2202906 words, keeping 51244 word types\n",
      "2018-07-03 18:40:06,796: INFO: PROGRESS: at sentence #20000, processed 4458315 words, keeping 67962 word types\n",
      "2018-07-03 18:40:07,420: INFO: PROGRESS: at sentence #30000, processed 6679221 words, keeping 81761 word types\n",
      "2018-07-03 18:40:08,062: INFO: PROGRESS: at sentence #40000, processed 8916893 words, keeping 93943 word types\n",
      "2018-07-03 18:40:08,896: INFO: PROGRESS: at sentence #50000, processed 11123478 words, keeping 103614 word types\n",
      "2018-07-03 18:40:09,569: INFO: PROGRESS: at sentence #60000, processed 13364359 words, keeping 112051 word types\n",
      "2018-07-03 18:40:10,133: INFO: PROGRESS: at sentence #70000, processed 15568436 words, keeping 119823 word types\n",
      "2018-07-03 18:40:10,451: INFO: collected 123477 word types from a corpus of 16693916 raw words and 75000 sentences\n",
      "2018-07-03 18:40:10,453: INFO: Loading a fresh vocabulary\n",
      "2018-07-03 18:40:11,055: INFO: min_count=40 retains 16464 unique words (13% of original 123477, drops 107013)\n",
      "2018-07-03 18:40:11,056: INFO: min_count=40 leaves 16134771 word corpus (96% of original 16693916, drops 559145)\n",
      "2018-07-03 18:40:11,189: INFO: deleting the raw counts dictionary of 123477 items\n",
      "2018-07-03 18:40:11,194: INFO: sample=0.001 downsamples 48 most-common words\n",
      "2018-07-03 18:40:11,195: INFO: downsampling leaves estimated 12250008 word corpus (75.9% of prior 16134771)\n",
      "2018-07-03 18:40:11,293: INFO: estimated required memory for 16464 words and 300 dimensions: 47745600 bytes\n",
      "2018-07-03 18:40:11,295: INFO: resetting layer weights\n",
      "2018-07-03 18:40:11,799: INFO: training model with 4 workers on 16464 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-07-03 18:40:12,808: INFO: EPOCH 1 - PROGRESS: at 2.85% examples, 337300 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:40:13,809: INFO: EPOCH 1 - PROGRESS: at 6.14% examples, 370745 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:14,820: INFO: EPOCH 1 - PROGRESS: at 9.25% examples, 373033 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:15,827: INFO: EPOCH 1 - PROGRESS: at 11.96% examples, 362329 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:16,863: INFO: EPOCH 1 - PROGRESS: at 14.67% examples, 352555 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:17,868: INFO: EPOCH 1 - PROGRESS: at 17.00% examples, 341718 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:18,874: INFO: EPOCH 1 - PROGRESS: at 19.75% examples, 341072 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:19,875: INFO: EPOCH 1 - PROGRESS: at 22.59% examples, 342582 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:20,887: INFO: EPOCH 1 - PROGRESS: at 25.54% examples, 344766 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:21,895: INFO: EPOCH 1 - PROGRESS: at 28.35% examples, 343924 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:22,899: INFO: EPOCH 1 - PROGRESS: at 31.05% examples, 342014 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:23,922: INFO: EPOCH 1 - PROGRESS: at 33.83% examples, 341496 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:24,984: INFO: EPOCH 1 - PROGRESS: at 36.52% examples, 339138 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:26,017: INFO: EPOCH 1 - PROGRESS: at 39.19% examples, 337664 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:40:27,042: INFO: EPOCH 1 - PROGRESS: at 41.86% examples, 337257 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:28,084: INFO: EPOCH 1 - PROGRESS: at 44.72% examples, 337020 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:40:29,127: INFO: EPOCH 1 - PROGRESS: at 47.65% examples, 337029 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:30,145: INFO: EPOCH 1 - PROGRESS: at 50.24% examples, 335601 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:31,167: INFO: EPOCH 1 - PROGRESS: at 52.49% examples, 332480 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:32,175: INFO: EPOCH 1 - PROGRESS: at 54.56% examples, 328374 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:33,196: INFO: EPOCH 1 - PROGRESS: at 57.21% examples, 327852 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:34,236: INFO: EPOCH 1 - PROGRESS: at 59.62% examples, 325796 words/s, in_qsize 5, out_qsize 2\n",
      "2018-07-03 18:40:35,252: INFO: EPOCH 1 - PROGRESS: at 61.85% examples, 322767 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:36,283: INFO: EPOCH 1 - PROGRESS: at 64.16% examples, 320971 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:37,284: INFO: EPOCH 1 - PROGRESS: at 66.65% examples, 320216 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:38,307: INFO: EPOCH 1 - PROGRESS: at 69.54% examples, 321462 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:39,323: INFO: EPOCH 1 - PROGRESS: at 72.35% examples, 322414 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:40,329: INFO: EPOCH 1 - PROGRESS: at 75.33% examples, 323883 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:41,354: INFO: EPOCH 1 - PROGRESS: at 78.45% examples, 325384 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:42,388: INFO: EPOCH 1 - PROGRESS: at 81.47% examples, 326354 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:43,428: INFO: EPOCH 1 - PROGRESS: at 84.29% examples, 326577 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:44,445: INFO: EPOCH 1 - PROGRESS: at 87.03% examples, 326369 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:45,452: INFO: EPOCH 1 - PROGRESS: at 89.83% examples, 326645 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:40:46,473: INFO: EPOCH 1 - PROGRESS: at 92.29% examples, 325727 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:47,473: INFO: EPOCH 1 - PROGRESS: at 94.37% examples, 323902 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:48,483: INFO: EPOCH 1 - PROGRESS: at 96.54% examples, 322275 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:49,486: INFO: EPOCH 1 - PROGRESS: at 98.90% examples, 321366 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-03 18:40:49,912: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-03 18:40:49,927: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-03 18:40:49,968: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-03 18:40:49,971: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-03 18:40:49,973: INFO: EPOCH - 1 : training on 16693916 raw words (12247736 effective words) took 38.2s, 320874 effective words/s\n",
      "2018-07-03 18:40:50,990: INFO: EPOCH 2 - PROGRESS: at 2.12% examples, 251422 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:52,023: INFO: EPOCH 2 - PROGRESS: at 4.39% examples, 262218 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:53,063: INFO: EPOCH 2 - PROGRESS: at 6.74% examples, 264924 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-03 18:40:54,085: INFO: EPOCH 2 - PROGRESS: at 8.85% examples, 262077 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:55,095: INFO: EPOCH 2 - PROGRESS: at 10.87% examples, 258403 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:56,096: INFO: EPOCH 2 - PROGRESS: at 12.82% examples, 255117 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:57,099: INFO: EPOCH 2 - PROGRESS: at 15.09% examples, 257761 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:58,102: INFO: EPOCH 2 - PROGRESS: at 17.70% examples, 265878 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:40:59,169: INFO: EPOCH 2 - PROGRESS: at 19.92% examples, 264811 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:00,170: INFO: EPOCH 2 - PROGRESS: at 22.25% examples, 267129 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:01,180: INFO: EPOCH 2 - PROGRESS: at 24.46% examples, 267468 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:02,256: INFO: EPOCH 2 - PROGRESS: at 26.50% examples, 265104 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:03,259: INFO: EPOCH 2 - PROGRESS: at 29.29% examples, 269626 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:04,266: INFO: EPOCH 2 - PROGRESS: at 31.69% examples, 271271 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:05,301: INFO: EPOCH 2 - PROGRESS: at 34.28% examples, 273912 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-03 18:41:06,330: INFO: EPOCH 2 - PROGRESS: at 36.35% examples, 272153 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-03 18:41:07,349: INFO: EPOCH 2 - PROGRESS: at 39.48% examples, 278525 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:08,393: INFO: EPOCH 2 - PROGRESS: at 42.55% examples, 283934 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:09,398: INFO: EPOCH 2 - PROGRESS: at 45.75% examples, 288860 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:10,410: INFO: EPOCH 2 - PROGRESS: at 48.90% examples, 293317 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:11,422: INFO: EPOCH 2 - PROGRESS: at 51.81% examples, 296362 words/s, in_qsize 5, out_qsize 2\n",
      "2018-07-03 18:41:12,453: INFO: EPOCH 2 - PROGRESS: at 54.40% examples, 296867 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:13,495: INFO: EPOCH 2 - PROGRESS: at 56.83% examples, 296533 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:41:14,495: INFO: EPOCH 2 - PROGRESS: at 59.57% examples, 297937 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:15,511: INFO: EPOCH 2 - PROGRESS: at 62.46% examples, 299651 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:16,542: INFO: EPOCH 2 - PROGRESS: at 65.28% examples, 300832 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:17,567: INFO: EPOCH 2 - PROGRESS: at 68.31% examples, 303458 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:18,606: INFO: EPOCH 2 - PROGRESS: at 71.23% examples, 305019 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:19,618: INFO: EPOCH 2 - PROGRESS: at 73.78% examples, 305293 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:20,619: INFO: EPOCH 2 - PROGRESS: at 76.35% examples, 305654 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:21,641: INFO: EPOCH 2 - PROGRESS: at 79.13% examples, 306302 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:22,692: INFO: EPOCH 2 - PROGRESS: at 81.74% examples, 306341 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:41:23,699: INFO: EPOCH 2 - PROGRESS: at 84.45% examples, 307032 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:24,722: INFO: EPOCH 2 - PROGRESS: at 86.79% examples, 305914 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:25,730: INFO: EPOCH 2 - PROGRESS: at 88.98% examples, 304709 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:26,737: INFO: EPOCH 2 - PROGRESS: at 91.08% examples, 303429 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:27,748: INFO: EPOCH 2 - PROGRESS: at 93.09% examples, 301806 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:28,756: INFO: EPOCH 2 - PROGRESS: at 95.32% examples, 301034 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:29,766: INFO: EPOCH 2 - PROGRESS: at 97.71% examples, 300845 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:30,648: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-03 18:41:30,679: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-03 18:41:30,688: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-03 18:41:30,744: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-03 18:41:30,747: INFO: EPOCH - 2 : training on 16693916 raw words (12250084 effective words) took 40.8s, 300541 effective words/s\n",
      "2018-07-03 18:41:31,764: INFO: EPOCH 3 - PROGRESS: at 2.25% examples, 264402 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:32,765: INFO: EPOCH 3 - PROGRESS: at 4.49% examples, 272817 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:33,800: INFO: EPOCH 3 - PROGRESS: at 7.01% examples, 279438 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:34,815: INFO: EPOCH 3 - PROGRESS: at 9.64% examples, 289485 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:35,832: INFO: EPOCH 3 - PROGRESS: at 12.24% examples, 294231 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:36,845: INFO: EPOCH 3 - PROGRESS: at 14.96% examples, 298792 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-03 18:41:37,897: INFO: EPOCH 3 - PROGRESS: at 17.94% examples, 306344 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-03 18:41:38,900: INFO: EPOCH 3 - PROGRESS: at 20.75% examples, 311023 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:39,924: INFO: EPOCH 3 - PROGRESS: at 23.60% examples, 314810 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:41:40,936: INFO: EPOCH 3 - PROGRESS: at 26.28% examples, 316723 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:41,946: INFO: EPOCH 3 - PROGRESS: at 29.02% examples, 317226 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:42,954: INFO: EPOCH 3 - PROGRESS: at 31.63% examples, 316998 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:43,962: INFO: EPOCH 3 - PROGRESS: at 34.28% examples, 317711 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:45,014: INFO: EPOCH 3 - PROGRESS: at 36.74% examples, 315524 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:41:46,025: INFO: EPOCH 3 - PROGRESS: at 38.82% examples, 311103 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:47,059: INFO: EPOCH 3 - PROGRESS: at 40.85% examples, 307354 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:48,107: INFO: EPOCH 3 - PROGRESS: at 43.04% examples, 304584 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:49,134: INFO: EPOCH 3 - PROGRESS: at 45.28% examples, 302139 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-03 18:41:50,144: INFO: EPOCH 3 - PROGRESS: at 47.64% examples, 301226 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:51,167: INFO: EPOCH 3 - PROGRESS: at 50.05% examples, 300608 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:52,172: INFO: EPOCH 3 - PROGRESS: at 52.30% examples, 299706 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:53,179: INFO: EPOCH 3 - PROGRESS: at 54.73% examples, 299408 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:54,205: INFO: EPOCH 3 - PROGRESS: at 57.40% examples, 300132 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:55,218: INFO: EPOCH 3 - PROGRESS: at 60.04% examples, 300938 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:56,239: INFO: EPOCH 3 - PROGRESS: at 62.63% examples, 301055 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:57,263: INFO: EPOCH 3 - PROGRESS: at 65.41% examples, 301954 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:58,273: INFO: EPOCH 3 - PROGRESS: at 68.25% examples, 303973 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:41:59,283: INFO: EPOCH 3 - PROGRESS: at 71.11% examples, 305543 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:00,318: INFO: EPOCH 3 - PROGRESS: at 73.89% examples, 306538 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:01,331: INFO: EPOCH 3 - PROGRESS: at 76.65% examples, 307430 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:02,343: INFO: EPOCH 3 - PROGRESS: at 79.33% examples, 307660 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:03,360: INFO: EPOCH 3 - PROGRESS: at 81.69% examples, 307092 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:04,376: INFO: EPOCH 3 - PROGRESS: at 84.34% examples, 307463 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:05,421: INFO: EPOCH 3 - PROGRESS: at 87.14% examples, 307796 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:06,426: INFO: EPOCH 3 - PROGRESS: at 89.69% examples, 307803 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:07,432: INFO: EPOCH 3 - PROGRESS: at 92.34% examples, 308179 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:08,481: INFO: EPOCH 3 - PROGRESS: at 94.43% examples, 306522 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:09,488: INFO: EPOCH 3 - PROGRESS: at 96.54% examples, 305269 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:10,497: INFO: EPOCH 3 - PROGRESS: at 98.53% examples, 303704 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:11,106: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-03 18:42:11,153: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-03 18:42:11,159: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-03 18:42:11,188: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-03 18:42:11,190: INFO: EPOCH - 3 : training on 16693916 raw words (12250978 effective words) took 40.4s, 302976 effective words/s\n",
      "2018-07-03 18:42:12,201: INFO: EPOCH 4 - PROGRESS: at 2.37% examples, 281357 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:13,213: INFO: EPOCH 4 - PROGRESS: at 4.62% examples, 279585 words/s, in_qsize 8, out_qsize 0\n",
      "2018-07-03 18:42:14,214: INFO: EPOCH 4 - PROGRESS: at 7.08% examples, 284800 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-03 18:42:15,228: INFO: EPOCH 4 - PROGRESS: at 9.42% examples, 284763 words/s, in_qsize 8, out_qsize 1\n",
      "2018-07-03 18:42:16,268: INFO: EPOCH 4 - PROGRESS: at 12.08% examples, 290416 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:17,270: INFO: EPOCH 4 - PROGRESS: at 14.78% examples, 296128 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:18,283: INFO: EPOCH 4 - PROGRESS: at 17.36% examples, 298590 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:19,292: INFO: EPOCH 4 - PROGRESS: at 19.97% examples, 301524 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:20,316: INFO: EPOCH 4 - PROGRESS: at 22.36% examples, 300062 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:21,334: INFO: EPOCH 4 - PROGRESS: at 24.75% examples, 299064 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:22,382: INFO: EPOCH 4 - PROGRESS: at 27.11% examples, 297398 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:23,397: INFO: EPOCH 4 - PROGRESS: at 29.64% examples, 297043 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:24,406: INFO: EPOCH 4 - PROGRESS: at 32.18% examples, 298178 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:25,429: INFO: EPOCH 4 - PROGRESS: at 34.73% examples, 298942 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:26,439: INFO: EPOCH 4 - PROGRESS: at 37.43% examples, 300451 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:27,478: INFO: EPOCH 4 - PROGRESS: at 40.01% examples, 301179 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:28,504: INFO: EPOCH 4 - PROGRESS: at 42.81% examples, 303766 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:29,508: INFO: EPOCH 4 - PROGRESS: at 45.69% examples, 305987 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:42:30,509: INFO: EPOCH 4 - PROGRESS: at 48.61% examples, 308478 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:31,517: INFO: EPOCH 4 - PROGRESS: at 51.30% examples, 309545 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:32,563: INFO: EPOCH 4 - PROGRESS: at 53.99% examples, 309922 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:42:33,573: INFO: EPOCH 4 - PROGRESS: at 56.54% examples, 310062 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:34,602: INFO: EPOCH 4 - PROGRESS: at 59.22% examples, 310254 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:35,614: INFO: EPOCH 4 - PROGRESS: at 62.14% examples, 311621 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:42:36,619: INFO: EPOCH 4 - PROGRESS: at 65.34% examples, 314626 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:37,622: INFO: EPOCH 4 - PROGRESS: at 68.66% examples, 318479 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:38,642: INFO: EPOCH 4 - PROGRESS: at 71.55% examples, 319766 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:39,664: INFO: EPOCH 4 - PROGRESS: at 74.25% examples, 319912 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:40,665: INFO: EPOCH 4 - PROGRESS: at 76.94% examples, 320287 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:41,676: INFO: EPOCH 4 - PROGRESS: at 79.71% examples, 320568 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:42,706: INFO: EPOCH 4 - PROGRESS: at 82.67% examples, 321735 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:43,718: INFO: EPOCH 4 - PROGRESS: at 85.86% examples, 323475 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:44,729: INFO: EPOCH 4 - PROGRESS: at 89.06% examples, 325098 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:45,738: INFO: EPOCH 4 - PROGRESS: at 92.11% examples, 326435 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:46,750: INFO: EPOCH 4 - PROGRESS: at 95.44% examples, 328747 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:47,772: INFO: EPOCH 4 - PROGRESS: at 98.91% examples, 331225 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:48,056: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-03 18:42:48,087: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-03 18:42:48,089: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-03 18:42:48,102: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-03 18:42:48,103: INFO: EPOCH - 4 : training on 16693916 raw words (12250990 effective words) took 36.9s, 331982 effective words/s\n",
      "2018-07-03 18:42:49,110: INFO: EPOCH 5 - PROGRESS: at 3.30% examples, 396233 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:50,130: INFO: EPOCH 5 - PROGRESS: at 6.68% examples, 399699 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:51,147: INFO: EPOCH 5 - PROGRESS: at 9.82% examples, 393973 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:52,151: INFO: EPOCH 5 - PROGRESS: at 12.88% examples, 387494 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:53,179: INFO: EPOCH 5 - PROGRESS: at 16.07% examples, 385772 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:54,210: INFO: EPOCH 5 - PROGRESS: at 19.22% examples, 384520 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:55,223: INFO: EPOCH 5 - PROGRESS: at 22.65% examples, 389616 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:56,225: INFO: EPOCH 5 - PROGRESS: at 26.00% examples, 392889 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:57,241: INFO: EPOCH 5 - PROGRESS: at 29.57% examples, 395999 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:58,260: INFO: EPOCH 5 - PROGRESS: at 32.85% examples, 395768 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:42:59,283: INFO: EPOCH 5 - PROGRESS: at 35.92% examples, 393636 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:00,300: INFO: EPOCH 5 - PROGRESS: at 39.03% examples, 392072 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:01,300: INFO: EPOCH 5 - PROGRESS: at 41.81% examples, 389203 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:02,306: INFO: EPOCH 5 - PROGRESS: at 45.04% examples, 389108 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:03,338: INFO: EPOCH 5 - PROGRESS: at 48.50% examples, 390115 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:04,343: INFO: EPOCH 5 - PROGRESS: at 51.75% examples, 390944 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:43:05,362: INFO: EPOCH 5 - PROGRESS: at 54.86% examples, 389957 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:43:06,368: INFO: EPOCH 5 - PROGRESS: at 58.37% examples, 391706 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:07,385: INFO: EPOCH 5 - PROGRESS: at 61.80% examples, 392387 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:08,399: INFO: EPOCH 5 - PROGRESS: at 65.29% examples, 393745 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:09,414: INFO: EPOCH 5 - PROGRESS: at 68.36% examples, 393210 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:10,440: INFO: EPOCH 5 - PROGRESS: at 71.49% examples, 392555 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:11,448: INFO: EPOCH 5 - PROGRESS: at 74.60% examples, 391928 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:12,472: INFO: EPOCH 5 - PROGRESS: at 77.70% examples, 391204 words/s, in_qsize 6, out_qsize 1\n",
      "2018-07-03 18:43:13,493: INFO: EPOCH 5 - PROGRESS: at 81.17% examples, 391891 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:14,499: INFO: EPOCH 5 - PROGRESS: at 84.63% examples, 393054 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:15,506: INFO: EPOCH 5 - PROGRESS: at 88.23% examples, 394183 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:16,509: INFO: EPOCH 5 - PROGRESS: at 91.55% examples, 394687 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:17,522: INFO: EPOCH 5 - PROGRESS: at 94.66% examples, 394119 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:18,529: INFO: EPOCH 5 - PROGRESS: at 97.77% examples, 393656 words/s, in_qsize 7, out_qsize 0\n",
      "2018-07-03 18:43:19,218: INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2018-07-03 18:43:19,229: INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2018-07-03 18:43:19,251: INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2018-07-03 18:43:19,270: INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2018-07-03 18:43:19,271: INFO: EPOCH - 5 : training on 16693916 raw words (12250116 effective words) took 31.2s, 393104 effective words/s\n",
      "2018-07-03 18:43:19,275: INFO: training on a 83469580 raw words (61249904 effective words) took 187.5s, 326710 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f2c10d3c4e0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "# logging.basicConfig(format=\"%(asctime)s: %(levelname)s: %(message)s\", level=logging.INFO)\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, \n",
    "                         window=context, sample=downsampling)\n",
    "\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
